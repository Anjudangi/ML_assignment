{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e15f34-53fa-4e60-a0ca-774dcc1bf74c",
   "metadata": {},
   "source": [
    "Q1. What is Elastic Net Regression and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01072942-41d4-44f2-b2ae-96780ef25253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans.1 Elastic Net Regression is a regularization technique that combines the penalties of both Ridge Regression and Lasso Regression in an attempt to leverage their respective strengths. Here‚Äôs a detailed explanation of Elastic Net Regression and its differences from other regression techniques:\n",
    "\n",
    "# 1. Purpose of Elastic Net Regression:\n",
    "# Elastic Net Regression is designed to address some limitations of Ridge Regression and Lasso Regression by combining their penalties. It aims to:\n",
    "\n",
    "# Handle multicollinearity (high correlation between predictors).\n",
    "# Perform feature selection by shrinking coefficients and setting some to exact zero.\n",
    "# Improve prediction accuracy by balancing bias and variance more effectively.\n",
    "\n",
    "# Key Differences from Other Regression Techniques:\n",
    "# Ridge Regression:\n",
    "\n",
    "# Adds a penalty term proportional to the square of the coefficients (L2 norm) to the cost function.\n",
    "# Encourages small but non-zero coefficients for all predictors, reducing the impact of multicollinearity by spreading coefficient values.\n",
    "# Lasso Regression:\n",
    "\n",
    "# Adds a penalty term proportional to the absolute value of the coefficients (L1 norm) to the cost function.\n",
    "# Promotes sparsity in the coefficient vector by shrinking less influential coefficients to exact zero, effectively performing feature selection.\n",
    "# Elastic Net Regression:\n",
    "\n",
    "# Combines both Lasso and Ridge penalties, aiming to leverage the advantages of both techniques.\n",
    "# Addresses the tendency of Lasso to select at most \n",
    "# n variables before it saturates, especially when \n",
    "# p is large or there is multicollinearity among predictors.\n",
    "# Provides a more flexible and robust approach than Ridge or Lasso alone in scenarios where predictors are highly correlated or when there are more predictors than observations.\n",
    "# 4. Advantages of Elastic Net Regression:\n",
    "# Handles Multicollinearity: Like Ridge Regression, Elastic Net Regression can handle multicollinearity by shrinking coefficients, but it can also perform variable selection like Lasso.\n",
    "# Improved Stability: Compared to Lasso, Elastic Net is more stable when predictors are highly correlated or when \n",
    "# p is large relative to n.\n",
    "# Flexible Tuning: The mixing parameter \n",
    "# Œ±=1), offering a continuum of solutions.\n",
    "# 5. Choosing Parameters in Elastic Net:\n",
    "# Cross-Validation: Similar to Ridge and Lasso, tuning parame n Elastic Net Regression often involves cross-validation techniques to find the optimal values that maximize model performance on unseen data.\n",
    "# Conclusion:\n",
    "# Elastic Net Regression represents a flexible and powerful regularization technique that combines the strengths of Ridge and Lasso Regression. It is particularly useful in scenarios where predictors exhibit multicollinearity or when feature selection is desired alongside regularization. By striking a balance between Ridge's ability to handle multicollinearity and Lasso's feature selection capabilities, Elastic Net offers a robust approach to linear regression modeling in various practical applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599de89-7f17-4476-b947-0b3ec23e55f6",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal values of the regularization parameters for Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d8e3a1-c988-4a10-af5f-9ec09abf85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans.2 Choosing the optimal values of the regularization parameters for Elastic Net Regression involves tuning three main parameters: \n",
    "# ùõº Here‚Äôs a structured approach to selecting these parameters:\n",
    "\n",
    "# 1. Understand the Parameters:\n",
    "# Œ± Mixing parameter that controls the balance between Lasso (L1 penalty) and Ridge (L2 penalty) regularization. It ranges from 0 to 1.\n",
    "# Œ±=0: Equivalent to Ridge Regression (only L2 penalty).\n",
    "# Œ±=1: Equivalent to Lasso Regression (only L1 penalty).\n",
    "# Values between 0 and 1 allow for a combination of both penalties.\n",
    "#  Regularization parameter for the Lasso penalty (L1 norm). It controls the strength of the penalty on the absolute value of the coefficients\n",
    "#  Regularization parameter for the Ridge penalty (L2 norm). It controls the strength of the penalty on the square of the coefficients.\n",
    "\n",
    "# Grid Search or Randomized Search:\n",
    "# Grid Search:\n",
    "\n",
    "# Define a grid of values forto explore.\n",
    "# Perform cross-validation (e.g., k-fold cross-validation) on the training data for each combination of parameters in the grid.\n",
    "# Evaluate model performance using a chosen metric (e.g., mean squared error, ùëÖ2 score).\n",
    "# Select the combination of parameters that yields the best cross-validation performance.\n",
    "# Randomized Search:\n",
    "\n",
    "# Define distributions (e.g., uniform or log-uniform) for a \n",
    "# Randomly sample a specified number of parameter combinations from these distributions.\n",
    "# Perform cross-validation and evaluate model performance as in grid search.\n",
    "# Choose the parameter combination that gives the best cross-validation performance.\n",
    "# 3. Cross-Validation:\n",
    "# Use cross-validation techniques to avoid overfitting and to estimate how the model will generalize to unseen data.\n",
    "# Split the dataset into training and validation sets multiple times (e.g., using k-fold cross-validation).\n",
    "# Compute the average performance metric across folds for each parameter combination to ensure robustness in parameter selection.\n",
    "# 4. Performance Metrics:\n",
    "# Select an appropriate performance metric based on the specific problem:\n",
    "# Regression: Mean squared error (MSE), \n",
    "# ùëÖ2 score.\n",
    "# Classification: Accuracy, F1 score, area under the ROC curve (AUC).\n",
    "# 5. Practical Considerations:\n",
    "# Dataset Size: Larger datasets may require finer granularity in parameter tuning due to increased variability.\n",
    "#Interpretability vs. Performance: Consider the trade-off between model interpretability (sparsity) and predictive performance when choosing Œ±.\n",
    "# Library Support: Utilize libraries like scikit-learn in Python, which provide tools such as GridSearchCV and RandomizedSearchCV for automating parameter tuning.\n",
    "# Conclusion:\n",
    "# Choosing the optimal values of the regularization parameters in Elastic Net Regression is crucial for achieving the best model performance. By systematically evaluating combinations of these parameters through grid search or randomized search and using cross-validation to assess performance, you can ensure that the Elastic Net model generalizes well to new data while effectively handling multicollinearity and performing feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660a7da-4854-4469-85e7-c6593c91533c",
   "metadata": {},
   "source": [
    "Q3. What are the advantages and disadvantages of Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4562c7-5d2a-4527-bde1-0dd79cc9f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ans.3  Advantages of Elastic Net Regression:\n",
    " # Combines Strengths of Lasso and Ridge:\n",
    "\n",
    "#  Elastic Net combines the penalties of both Lasso and Ridge regression, making it more versatile. It performs well in situations where predictors are highly correlated or when the number of predictors exceeds the number of observations.\n",
    "#  Feature Selection:\n",
    "\n",
    "# Like Lasso, Elastic Net can shrink some coefficients to exactly zero, which helps in automatic feature selection. This is beneficial for simplifying models and enhancing interpretability.\n",
    "#  Handles Multicollinearity:\n",
    "\n",
    "#  Elastic Net is particularly effective in handling multicollinearity among predictors. By incorporating the Ridge penalty, it can distribute the effect among correlated predictors, which reduces variance.\n",
    "#  Flexibility in Regularization:\n",
    "\n",
    "# The mixing parameter \n",
    "#  Œ± allows for a flexible combination of Lasso (L1) and Ridge (L2) penalties. This flexibility helps in fine-tuning the regularization to achieve the best model performance.\n",
    "#  Improved Prediction Accuracy:\n",
    "\n",
    "#  By balancing bias and variance more effectively, Elastic Net often results in better prediction accuracy compared to using Lasso or Ridge alone, especially in complex datasets.\n",
    "# Disadvantages of Elastic Net Regression:\n",
    "# Complexity in Parameter Tuning:\n",
    "\n",
    "#  Elastic Net requires tuning of multiple parameters , which can be computationally intensive and complex. This process often requires extensive cross-validation and grid search techniques.\n",
    "#  Interpretability:\n",
    "\n",
    "#  While Elastic Net performs feature selection, the presence of both L1 and L2 penalties can make the interpretation of the final model coefficients less straightforward compared to pure Lasso regression.\n",
    "#  Computational Cost:\n",
    "\n",
    "#  The additional computational cost due to the need to tune multiple regularization parameters can be a limitation, particularly for very large datasets or high-dimensional data.\n",
    "#  Sensitivity to Data Scaling:\n",
    "\n",
    "#  Elastic Net, like other regularization techniques, is sensitive to the scaling of features. Proper standardization or normalization of the predictors is essential to ensure the penalties are applied uniformly across all features.\n",
    "#  Potential for Over-regularization:\n",
    "\n",
    "#  If the regularization parameters are not properly tuned, there is a risk of over-regularization, which can lead to underfitting. This would result in a model that fails to capture the underlying patterns in the data adequately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109acb69-9bc9-4c41-9936-0d28962c6aca",
   "metadata": {},
   "source": [
    "Q4. What are some common use cases for Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368cba22-d737-4e08-a484-1e2a6467db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.4 Elastic Net Regression is particularly useful in various scenarios, especially where high-dimensional data and multicollinearity are present. Here are some common use cases for Elastic Net Regression:\n",
    "\n",
    "# 1. Genomics and Bioinformatics:\n",
    "#  Gene Expression Data: When dealing with gene expression data, where the number of genes (features) is often much larger than the number of samples, Elastic Net helps in selecting relevant genes while managing multicollinearity among gene expressions.\n",
    "# Genetic Association Studies: Elastic Net can identify genetic markers associated with diseases by handling the high-dimensional nature and correlations among genetic data.\n",
    "#  2. Finance and Economics:\n",
    "# Stock Price Prediction: In financial modeling, where numerous economic indicators and stock features may influence stock prices, Elastic Net can select significant predictors and mitigate multicollinearity among economic indicators.\n",
    "#  Credit Scoring: Elastic Net helps in building credit scoring models by selecting important features from a large set of potential predictors, improving the model's robustness and accuracy.\n",
    "# 3. Marketing and Customer Analytics:\n",
    "# Customer Segmentation: Elastic Net can be used to segment customers based on various demographic, behavioral, and transactional data, ensuring that only relevant features are considered.\n",
    "# Predictive Marketing: By analyzing a vast amount of marketing data, Elastic Net can identify key factors that influence customer behavior, helping in targeted marketing campaigns and improving customer retention.\n",
    "# 4. Healthcare and Medical Research:\n",
    "#  Disease Prediction: Elastic Net can be applied to predict the risk of diseases by selecting relevant clinical and genetic features from a large dataset, aiding in early diagnosis and personalized treatment plans.\n",
    "#  Health Outcome Modeling: In studies where multiple health metrics are recorded, Elastic Net helps in identifying the most significant predictors of health outcomes, improving the accuracy of predictive models.\n",
    "# 5. Natural Language Processing (NLP):\n",
    "#  Text Classification: In text classification tasks, where a large number of features (words or phrases) are used, Elastic Net can effectively select relevant features, improving the model‚Äôs performance and interpretability.\n",
    "#  Sentiment Analysis: Elastic Net can help in identifying the most influential words or phrases that determine sentiment in a large corpus of text data.\n",
    "#  6. Environmental Science:\n",
    "\n",
    "#  Climate Modeling: Elastic Net can be used to model and predict climate changes by handling a large number of climate variables and their correlations, providing more accurate and robust predictions.\n",
    "#  Pollution Analysis: In studies analyzing pollution data, Elastic Net can identify key factors contributing to pollution levels from a large set of potential predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb4b795-0d6b-4620-b326-9cdd7700a3a7",
   "metadata": {},
   "source": [
    "Q5. How do you interpret the coefficients in Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeff59eb-ae2a-4a23-82f6-d31ec1602262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans. 5Interpreting the coefficients in Elastic Net Regression involves understanding both the magnitude and the sign of the coefficients, as well as the effects of the regularization terms. Here‚Äôs a detailed explanation:\n",
    "\n",
    "# 1. Magnitude and Sign of Coefficients:\n",
    "# Magnitude: The absolute value of the coefficient indicates the strength of the relationship between the independent variable and the dependent variable. Larger magnitudes suggest a stronger influence on the dependent variable.\n",
    "# Sign: The sign of the coefficient indicates the direction of the relationship. A positive coefficient means that as the independent variable increases, the dependent variable also increases, while a negative coefficient indicates an inverse relationship.\n",
    "# 2. Effect of Regularization:\n",
    "# Elastic Net Regression combines Lasso (L1) and Ridge (L2) penalties. The L1 penalty can shrink some coefficients to exactly zero, effectively performing feature selection. The L2 penalty shrinks coefficients towards zero but does not eliminate them entirely.\n",
    "# As a result, coefficients that are zero have been deemed less important by the model, indicating that these features are not contributing significantly to the prediction.\n",
    "# Coefficients that are non-zero have been identified as important predictors.\n",
    "# 3. Relative Importance:\n",
    "# The relative magnitude of the non-zero coefficients can be compared to assess the relative importance of different features. However, it‚Äôs important to note that the actual values of coefficients depend on the scale of the features. Hence, features should be standardized before fitting the model to make meaningful comparisons.\n",
    "# 4. Interpretation in Context of Domain:\n",
    "# Understanding the practical meaning of the coefficients requires domain knowledge. For example, in a healthcare setting, a positive coefficient for a certain biomarker might indicate an increased risk of a disease, while a negative coefficient might indicate a protective effect.\n",
    "# 5. Interactions and Multicollinearity:\n",
    "# Elastic Net can handle multicollinearity by distributing the effect among correlated predictors. However, if two variables are highly correlated, their individual coefficients might be smaller due to the shared explanatory power. Interpret the coefficients with caution in such cases.\n",
    "# Example:\n",
    "# Consider a scenario where Elastic Net Regression is used to predict house prices based on features such as square footage, number of bedrooms, and distance to the city center.\n",
    "\n",
    "# If the coefficient for square footage is 50, it means that for every additional square foot, the house price increases by $50, holding other variables constant.\n",
    "# If the coefficient for the number of bedrooms is 20,000, it indicates that each additional bedroom adds $20,000 to the house price, holding other variables constant.\n",
    "# If the coefficient for distance to the city center is -5,000, it suggests that for each mile closer to the city center, the house price increases by $5,000, holding other variables constant.\n",
    "# If the coefficient for another feature, say the age of the house, is zero, it means that this feature was not found to be significant in predicting house prices, and hence, it has been excluded by the model.\n",
    "# Conclusion:\n",
    "#Interpreting the coefficients in Elastic Net Regression involves analyzing the magnitude and sign of the coefficients while considering the effects of L1 and L2 regularization. The non-zero coefficients represent the selected important features, with their magnitudes indicating the strength of their influence on the dependent variable. Proper standardization and domain knowledge are crucial for meaningful interpretation of these coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a3dbd-e914-488d-b51c-34497dff662c",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values when using Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e558be-fd49-4478-9d73-9bdd1bc8c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans.6  Handling missing values is an essential preprocessing step before applying Elastic Net Regression. Here are several strategies to address missing values:\n",
    "\n",
    "# 1. Remove Rows with Missing Values:\n",
    "# Description: If the dataset is large and the number of rows with missing values is small, you might simply remove those rows.\n",
    "# Pros: Simple and straightforward.\n",
    "# Cons: Potential loss of valuable data, especially if many rows have missing values.\n",
    "#2. Remove Columns with Missing Values:\n",
    "# Description: If certain columns have a high proportion of missing values, you might remove those columns.\n",
    "# Pros: Useful when the feature is not critical or has too many missing values to impute reliably.\n",
    "# Cons: Loss of potentially valuable features.\n",
    "# 3. Imputation:\n",
    "# Mean/Median/Mode Imputation:\n",
    "\n",
    "# Description: Replace missing values with the mean, median, or mode of the column.\n",
    "# Pros: Simple and effective for numerical data.\n",
    "# Cons: Can introduce bias and does not account for the relationships between features.\n",
    "# K-Nearest Neighbors (KNN) Imputation:\n",
    "\n",
    "# Description: Replace missing values with the mean/median of the k-nearest neighbors.\n",
    "# Pros: More sophisticated, considers the relationships between features.\n",
    "# Cons: Computationally intensive, especially for large datasets.\n",
    "# Multivariate Imputation by Chained Equations (MICE):\n",
    "\n",
    "# Description: Imputes missing values by modeling each feature with missing values as a function of other features.\n",
    "# Pros: Accounts for the relationships between features, provides more accurate imputations.\n",
    "# Cons: Computationally intensive, requires careful implementation.\n",
    "# Regression Imputation:\n",
    "\n",
    "# Description: Use regression models to predict and impute missing values based on other features.\n",
    "# Pros: Accounts for relationships between features.\n",
    "# Cons: Requires creating and fitting multiple regression models, can be complex.\n",
    "# 4. Use Algorithms that Handle Missing Values:\n",
    "# Some machine learning algorithms can handle missing values internally, but Elastic Net Regression is not one of them. Therefore, preprocessing to handle missing values is necessary before fitting an Elastic Net model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2190d-ba22-4e18-9b85-fffed99bdd72",
   "metadata": {},
   "source": [
    "Q7. How do you use Elastic Net Regression for feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac61bbad-e0cf-4209-a59c-522993fc7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.7 Elastic Net Regression can be effectively used for feature selection due to its combination of L1 (Lasso) and L2 (Ridge) regularization penalties. The L1 regularization component helps in shrinking some coefficients to exactly zero, which implies those features are less important or redundant. Here's how you can use Elastic Net Regression for feature selection:\n",
    "\n",
    "# Steps for Using Elastic Net Regression for Feature Selection\n",
    "# Standardize the Data:\n",
    "\n",
    "# Standardizing the data ensures that all features are on the same scale, which is important for regularization techniques.\n",
    "\n",
    "#Fit the Elastic Net Model:\n",
    "\n",
    "# Fit the Elastic Net model to the data using a chosen alpha (regularization strength) and l1_ratio (mixing parameter between Lasso and Ridge).\n",
    "\n",
    "# Identify Non-Zero Coefficients:\n",
    "\n",
    "# The features with non-zero coefficients are considered important. The coefficients of the fitted Elastic Net model can be used to identify these features.\n",
    "# Use the identified important features to create a new dataset with only these features.\n",
    "  \n",
    "  # Tuning the Regularization Parameters\n",
    "# Choosing the optimal alpha and l1_ratio values is crucial for effective feature selection. This can be done using cross-validation techniques like Grid Search CV.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00edf28-a0f4-4c7c-a80b-4f8ab18def62",
   "metadata": {},
   "source": [
    "Q8. How do you pickle and unpickle a trained Elastic Net Regression model in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5429e57-183a-4ee1-85fd-7fbb6c541da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.8 Pickling and unpickling a trained Elastic Net Regression model in Python involves serializing the model to a file and then deserializing it when needed. This is useful for saving the model after training and loading it later for making predictions without retraining.\n",
    "\n",
    "# Steps to Pickle and Unpickle a Trained Elastic Net Regression Model\n",
    "# 1. Train the Elastic Net Regression Model\n",
    "# First, train the Elastic Net Regression model.\n",
    "\n",
    "# Pickle the Model\n",
    "# Use the pickle module to serialize the trained model to a file\n",
    "\n",
    "#  Unpickle the Model\n",
    "#  Use the pickle module to deserialize the model from the file.\n",
    "\n",
    "#  Use the Unpickled Model for Predictions\n",
    "#  Now that the model is loaded, you can use it to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa48b8-b05d-4e6c-b646-2317007eaacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26212d8-703e-4408-a4bc-1dc9754192b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans.9 The purpose of pickling a model in machine learning is to save a trained model to a file so that it can be easily loaded and used later without needing to retrain it. This process, known as serialization, converts the model object into a byte stream that can be stored on disk. Unpickling, or deserialization, converts the byte stream back into the original model object. Here are some key benefits of pickling a model:\n",
    "\n",
    "#  Benefits of Pickling a Model\n",
    "#  Efficiency:\n",
    "\n",
    "#  Time-Saving: Retraining a model can be time-consuming, especially for complex models or large datasets. Pickling allows you to save the trained model and reuse it without retraining, saving significant time.\n",
    "#  Resource-Saving: Training models can also be resource-intensive, requiring substantial computational power. Pickling avoids the need to reallocate resources for retraining.\n",
    "# Deployment:\n",
    "\n",
    "#  Model Deployment: When deploying a machine learning model into a production environment, you typically need to load a pre-trained model to make predictions. Pickling allows you to easily deploy models by loading the saved file.\n",
    "#  Consistency: Pickling ensures that the model used in production is the same as the one that was trained and validated, avoiding discrepancies that might arise from retraining.\n",
    "#  Reproducibility:\n",
    "\n",
    "#  Exact Reproduction: Pickling preserves the exact state of the model, including its learned parameters and hyperparameters, ensuring that you can reproduce results exactly.\n",
    "#  Experiment Tracking: When experimenting with different models, you can save each trained model to track and compare their performance later.\n",
    "#  Portability:\n",
    "\n",
    "#  Model Sharing: Pickled models can be easily shared between different systems or team members. This is particularly useful in collaborative projects where team members may need to work with the same model.\n",
    "#  Cross-Environment Use: Pickled models can be used across different environments (e.g., from a local development environment to a cloud-based production environment).\n",
    "#  Example Scenarios\n",
    "Model Deployment:\n",
    "\n",
    "After training a model locally, you pickle it and then load it into a web application for real-time predictions.\n",
    "Model Versioning:\n",
    "\n",
    "During the model development phase, you may train multiple versions of a model with different hyperparameters. Pickling each version allows you to compare their performance and select the best one.\n",
    "Experiment Reproduction:\n",
    "\n",
    "In a research setting, you can pickle models to ensure that experiments can be exactly reproduced by others or at a later time.\n",
    "Conclusion\n",
    "Pickling a model is a practical approach in machine learning for saving trained models, facilitating efficient deployment, ensuring reproducibility, and enabling easy sharing and portability of models. It streamlines the workflow by allowing you to save and reuse models without the need for retraining, ensuring consistency and efficiency in the model lifecycle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
