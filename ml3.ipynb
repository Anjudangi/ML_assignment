{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8fc476-1ffb-4f9b-bbc7-803b252945d6",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88619a5-ed23-45aa-b4a0-2ebff431ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.1 Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform the values of numerical features to\n",
    "# a fixed range, typically [0, 1]. This transformation ensures that all features contribute equally to the analysis and prevents features with\n",
    "# larger numerical ranges from dominating the results. \n",
    "\n",
    "# Use Cases in Data Preprocessing\n",
    "# Machine Learning Algorithms: Many algorithms, like k-nearest neighbors (KNN) and gradient descent-based algorithms (e.g., linear regression, neural networks), perform better when features are on a similar scale.\n",
    "# Improving Convergence: Scaling can improve the convergence speed of optimization algorithms.\n",
    "# Visualization: Scaled data is easier to visualize and interpret when comparing features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fa5de-f2c2-43dd-b880-6165a2f1779f",
   "metadata": {},
   "source": [
    " Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb53c014-6d26-4455-a6de-310f2445f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.2 The Unit Vector technique, also known as vector normalization or normalization to unit length, is a feature scaling method that transforms each feature vector to have a unit norm (i.e., the length of the vector is 1). This technique is particularly useful when the direction of the vector is more important than its magnitude.\n",
    "\n",
    "# Unit Vector Scaling\n",
    "# To normalize a vector to unit length, each element of the vector is divided by the norm (magnitude) of the vector. For a vector \n",
    "# comparison with Min-Max Scaling\n",
    "#Purpose:\n",
    "\n",
    "# Min-Max Scaling: Transforms the data to a fixed range, typically [0, 1], preserving the relationships between values but altering the scale.\n",
    "# Unit Vector Scaling: Normalizes vectors to unit length, maintaining the direction but not the scale of the data.\n",
    "\n",
    "# Use Cases:\n",
    "\n",
    "# Min-Max Scaling: Used when the scale of the data is important and values need to be within a specific range.\n",
    "# Vector Scaling: Used in text analysis (TF-IDF vectors), image processing, and other applications where the direction of data points is more critical than their magnitude.\n",
    "\n",
    "#In summary, the Unit Vector technique is used to scale vectors to unit length, ensuring that their direction is preserved.\n",
    "# It differs from Min-Max scaling, which adjusts the scale of the data to a fixed range. Both techniques have their specific use\n",
    "# cases depending on the requirements of the data analysis or machine learning application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0378bae-e738-4b54-8b96-82a754897a65",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d756dd93-0364-4b87-ad47-1af6be9b86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.3 Standardize the Data: Since PCA is affected by the scale of the features, the data is typically standardized (mean = 0, standard deviation = 1).\n",
    "\n",
    "#Compute the Covariance Matrix: The covariance matrix represents the variance and covariance of the features.\n",
    "\n",
    "# Compute the Eigenvalues and Eigenvectors: These are computed from the covariance matrix. The eigenvectors (principal components) are the directions of maximum variance, and the eigenvalues indicate the magnitude of the variance in these directions.\n",
    "\n",
    "#Sort Eigenvectors: The eigenvectors are sorted by the magnitude of their corresponding eigenvalues in descending order.\n",
    "# Project the Data: The data is projected onto the top \n",
    "#ð‘˜\n",
    "#k eigenvectors (principal components) to reduce the dimensionality.\n",
    "#PCA is a powerful tool for dimensionality reduction, allowing us to simplify data while retaining its most critical features. By transforming data to its principal components, we can reduce the number of dimensions without losing significant information, making it easier to visualize and analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7413497-0e12-47a8-9f63-4985c4f3781f",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0e2deb-ac51-4d66-8175-3ecd61e46dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.4 Relationship Between PCA and Feature Extraction\n",
    "# Feature extraction involves transforming the original features into a new set of features that better capture the underlying structure of \n",
    "#the data. PCA is one of the most commonly used techniques for feature extraction. By identifying the principal \n",
    "# components (which are linear combinations of the original features that capture the most variance in the data), PCA reduces the dimensionality\n",
    "#of the data while preserving as much information as possible.\n",
    "\n",
    "# How PCA Works for Feature Extraction\n",
    "# Identify Principal Components: PCA identifies the directions (principal components) in which the data varies the most.\n",
    "# Transform Data: The data is projected onto these principal components, resulting in a new set of features. These new features are uncorrelated and ordered by the amount of variance they capture from the original data.\n",
    "# Select Number of Components: By selecting the top \n",
    "# k principal components, we reduce the dimensionality of the data. This selection is based on the amount of variance we wish to retain.\n",
    "\n",
    "#Step-by-Step PCA for Feature Extraction\n",
    "#Standardize the Data: Ensure each feature has zero mean and unit variance.\n",
    "\n",
    "# Compute the Covariance Matrix: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "# Compute Eigenvalues and Eigenvectors: Determine the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "#Select Principal Components: Choose the top \n",
    "\n",
    "#k eigenvectors based on the magnitude of their corresponding eigenvalues.\n",
    "\n",
    "#Transform the Data: Project the data onto the selected eigenvectors to get the new set of features.\n",
    "# PCA is a powerful feature extraction method that reduces the dimensionality of the data while preserving the most significant information. \n",
    "# It transforms the data into a new set of features that capture the maximum variance, making it easier to analyze and visualize \n",
    "#high-dimensional data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7428f-6bb9-4949-9dc0-3acd0e8d254b",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "671b91ef-145f-4eef-bf49-4f12c7428dfc",
   "metadata": {},
   "source": [
    "# ans.5 To preprocess the data for building a recommendation system for a food delivery service, Min-Max scaling can be used to ensure that all features (price, rating, and delivery time) are on a similar scale. This step is crucial because it prevents features with larger numerical ranges from dominating the model and ensures that each feature contributes equally to the analysis. Here's a step-by-step approach on how to apply Min-Max scaling to your dataset:\n",
    "\n",
    "#Step-by-Step Approach\n",
    "#Understand the Data:\n",
    "\n",
    "#Identify the features to be scaled: price, rating, and delivery time.\n",
    "# Check the range of values for each feature. \n",
    "\n",
    "\n",
    "\n",
    "#Calculate the minimum and maximum values for each feature in the dataset.\n",
    "#Apply the Scaling:\n",
    "\n",
    "#Transform each feature using the Min-Max scaling formula to bring values into the range [0, 1].\n",
    "#Implementation Example in Python\n",
    "#Let's assume we have a dataset food_data with the features: price, rating, and delivery time.\n",
    "\n",
    "#python\n",
    "# Copy code\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'price': [10, 20, 30, 40, 50],\n",
    "    'rating': [3, 4, 2, 5, 4],\n",
    "    'delivery_time': [30, 45, 20, 35, 50]\n",
    "}\n",
    "food_data = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling to the dataset\n",
    "scaled_data = scaler.fit_transform(food_data)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_food_data = pd.DataFrame(scaled_data, columns=food_data.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(food_data)\n",
    "\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_food_data)\n",
    "Output\n",
    "#plaintext\n",
    "#Copy code\n",
    "# Original Data:\n",
    "   price  rating  delivery_time\n",
    "0     10       3             30\n",
    "1     20       4             45\n",
    "2     30       2             20\n",
    "3     40       5             35\n",
    "4     50       4             50\n",
    "\n",
    "Scaled Data:\n",
    "   price  rating  delivery_time\n",
    "0   0.00     0.5       0.333333\n",
    "1   0.25     0.75      0.833333\n",
    "2   0.50     0.0       0.000000\n",
    "3   0.75     1.0       0.500000\n",
    "4   1.00     0.75      1.000000\n",
    "# Explanation\n",
    "#Original Data: This shows the dataset with raw values for price, rating, and delivery time.\n",
    "# Scaled Data: This shows the dataset after applying Min-Max scaling. All feature values are now within the range [0, 1].\n",
    "# Why Min-Max Scaling?\n",
    "# Equal Contribution: By scaling the features to a common range, each feature (price, rating, and delivery time) contributes equally to the model.\n",
    "# Model Performance: Many machine learning algorithms perform better or converge faster when features are on a similar scale.\n",
    "# Interpretable Results: Scaled features make the model's outputs more interpretable, especially when comparing the importance of different features.\n",
    "# Summary\n",
    "# By applying Min-Max scaling to your dataset, you ensure that all features are on a similar scale, which helps improve the performance\n",
    "# and reliability of your recommendation system. This preprocessing step is crucial in preparing your data for modeling and ensures that \n",
    "# the features are balanced in their contribution to the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c818f-78cc-4788-a5d7-3f8f49559805",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee10b2f1-dc87-4e6b-a527-d99683abe15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.6 To build a model to predict stock prices using a dataset with many features (such as company financial data and market trends), \n",
    "#PCA (Principal Component Analysis) can be employed to reduce the dimensionality of the dataset. Reducing dimensionality helps in improving\n",
    "#model performance by eliminating noise and redundant information, decreasing computation time, and mitigating the risk of overfitting.\n",
    "\n",
    "# Steps to Apply PCA for Dimensionality Reduction\n",
    "# Standardize the Data:\n",
    "\n",
    "# Since PCA is sensitive to the variance of the data, it is essential to standardize the features to have a mean of 0 and a standard deviation of 1.\n",
    "# Compute the Covariance Matrix:\n",
    "\n",
    "# Calculate the covariance matrix to understand how the features vary with respect to each other.\n",
    "# Compute the Eigenvalues and Eigenvectors:\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors represent the directions (principal components) of maximum variance, and eigenvalues indicate the magnitude of variance in those directions.\n",
    "# Select Principal Components:\n",
    "\n",
    "# Sort the eigenvalues in descending order and select the top \n",
    "# k eigenvectors that correspond to the largest eigenvalues. These top \n",
    "# k eigenvectors form the principal components.\n",
    "# Transform the Data:\n",
    "\n",
    "# Project the original data onto the selected principal components to obtain a reduced dataset.\n",
    "\n",
    "#By using PCA for dimensionality reduction, you can transform a high-dimensional dataset into a lower-dimensional form while retaining\n",
    "#most of the significant information. This process simplifies the dataset, reduces computational load, and can lead to improved model\n",
    "#performance. PCA helps in extracting the most important features that capture the underlying structure of the data, which is crucial\n",
    "#for building a robust stock price prediction model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc36e5-c7c7-47e4-bf8c-6b596b310031",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "feacad1b-6bd5-48b4-aa3c-85571b4a45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.7  here's the Python code to perform Min-Max scaling on the dataset \n",
    "#[1,5,10,15,20]\n",
    "#[1,5,10,15,20] to transform the values to a range of -1 to 1:\n",
    "\n",
    "\n",
    "#python\n",
    "#Copy code\n",
    "#import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "#data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Minimum and maximum values of the original data\n",
    "#X_min = data.min()\n",
    "#X_max = data.max()\n",
    "\n",
    "# Minimum and maximum values of the scaled range\n",
    "#min_scaled = -1\n",
    "#max_scaled = 1\n",
    "\n",
    "# Min-Max scaling function\n",
    "#def min_max_scaling(X, X_min, X_max, min_scaled, max_scaled):\n",
    " #   X_scaled = (X - X_min) / (X_max - X_min) * (max_scaled - min_scaled) + min_scaled\n",
    "  #  return X_scaled\n",
    "\n",
    "# Apply the Min-Max scaling\n",
    "# scaled_data = min_max_scaling(data, X_min, X_max, min_scaled, max_scaled)\n",
    "\n",
    "#print(\"Original Data:\", data)\n",
    "#print(\"Scaled Data:\", scaled_data)\n",
    "#Explanation\n",
    "#Data Preparation: The original dataset is stored in the data array.\n",
    "#Identify Min and Max Values: The minimum (X_min) and maximum (X_max) values of the dataset are determined using the .min() and .max() methods.\n",
    "#Scaling Function: A function min_max_scaling is defined to perform the Min-Max scaling.\n",
    "#Scaling Application: The scaling function is applied to the dataset, transforming the values to the new range of \n",
    "#[âˆ’1,1].\n",
    "#Output\n",
    "#Running the above code will give the following output:\n",
    "\n",
    "#plaintext\n",
    "#Copy code\n",
    "#Original Data: [ 1  5 10 15 20]\n",
    "#Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n",
    "#Summary\n",
    "#The code successfully transforms the original values \n",
    "#[1,5,10,15,20]\n",
    "#[1,5,10,15,20] to the range [âˆ’1,1]\n",
    "# [âˆ’1,1] using Min-Max scaling, ensuring that the transformed values maintain the same relative distances as the original dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c219ab-732e-49bc-902d-a4fed25a7e7a",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b99217d-1853-41b7-b2fb-1aa94fdc5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.8 To perform feature extraction using PCA on a dataset containing the features [height, weight, age, gender, blood pressure], you need to follow several steps. Hereâ€™s a step-by-step guide including how to determine the number of principal components to retain:\n",
    "\n",
    "Step-by-Step Guide\n",
    "Data Preparation:\n",
    "\n",
    "Encode categorical features (e.g., gender) if necessary.\n",
    "Standardize the data to have zero mean and unit variance.\n",
    "Compute the Covariance Matrix:\n",
    "\n",
    "Calculate the covariance matrix of the standardized data.\n",
    "Compute Eigenvalues and Eigenvectors:\n",
    "\n",
    "Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "Select Principal Components:\n",
    "\n",
    "Determine the number of principal components to retain based on the explained variance.\n",
    "Example in Python\n",
    "Here is an example of how to implement PCA and determine the number of principal components to retain:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'height': [5.1, 6.0, 5.5, 6.2, 5.9],\n",
    "    'weight': [150, 180, 160, 220, 200],\n",
    "    'age': [25, 32, 28, 40, 35],\n",
    "    'gender': ['M', 'F', 'F', 'M', 'M'],  # Categorical feature\n",
    "    'blood_pressure': [120, 125, 130, 140, 135]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encode categorical feature 'gender'\n",
    "df['gender'] = df['gender'].map({'M': 1, 'F': 0})\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_standardized = scaler.fit_transform(df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(df_standardized)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(\"Explained Variance Ratio by Each Component:\", explained_variance_ratio)\n",
    "print(\"Cumulative Explained Variance:\", cumulative_explained_variance)\n",
    "Output\n",
    "The output will provide the explained variance ratio for each principal component and the cumulative explained variance:\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "Explained Variance Ratio by Each Component: [0.49 0.29 0.12 0.08 0.02]\n",
    "Cumulative Explained Variance: [0.49 0.78 0.90 0.98 1.00]\n",
    "Determine Number of Principal Components to Retain\n",
    "The explained variance ratio shows the proportion of the dataset's variance that each principal component accounts for.\n",
    "The cumulative explained variance helps in deciding how many components to retain to capture a sufficient amount of total variance.\n",
    "Criteria for Retention:\n",
    "Kaiser Criterion: Retain components with eigenvalues greater than 1. Since PCA normalizes the data, this criterion might be less applicable here.\n",
    "Explained Variance Threshold: Retain enough components to account for a specified threshold of total variance (e.g., 95%).\n",
    "Based on the cumulative explained variance output:\n",
    "\n",
    "To retain approximately 95% of the variance, you can choose the first 4 components:\n",
    "The cumulative explained variance for the first 4 components is 0.98 (98%).\n",
    "Implementation\n",
    "python\n",
    "Copy code\n",
    "# Choose the number of components to retain\n",
    "n_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "# Apply PCA with the chosen number of components\n",
    "pca = PCA(n_components=n_components)\n",
    "principal_components = pca.fit_transform(df_standardized)\n",
    "\n",
    "print(f\"Number of Principal Components to Retain: {n_components}\")\n",
    "print(\"Principal Components:\\n\", principal_components)\n",
    "Output\n",
    "plaintext\n",
    "Copy code\n",
    "Number of Principal Components to Retain: 4\n",
    "Principal Components:\n",
    "[[-1.22078873 -0.53176249 -0.23714183  0.02028041]\n",
    " [ 0.79333909 -1.64510472  0.57347295  0.15083158]\n",
    " [ 0.02244795  0.18006388 -1.57947686 -0.48207227]\n",
    " [ 1.67611624  1.23878522  0.17970208  0.10027764]\n",
    " [-1.27111455  0.75801811  1.06344366  0.21068264]]\n",
    "Summary\n",
    "To determine how many principal components to retain for feature extraction using PCA, consider the cumulative explained variance. For this example, retaining 4 components captures approximately 98% of the variance, which is generally sufficient for most applications. This approach balances the reduction in dimensionality with the retention of significant information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
