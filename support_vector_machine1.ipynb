{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3259f4-f232-4f3b-bc26-811eeeb16798",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0331635-bc1c-4924-b56e-09fac3ab8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans.1 The mathematical formulation of a linear Support Vector Machine (SVM) involves finding a hyperplane that best separates the data \n",
    "# points into different classes. For a binary classification problem, the linear SVM aims to find a decision boundary that maximizes the \n",
    "# margin between the two classes.\n",
    "\n",
    "# 1. Hyperplane Equation:\n",
    "# For a linear SVM, the decision boundary (hyperplane) can be represented as:\n",
    "# w⋅x+b=0\n",
    "# where:\n",
    "\n",
    "# w is the weight vector.\n",
    "# x is the input feature vector.\n",
    "# b is the bias term.\n",
    "\n",
    "# The classification decision for a data point \n",
    "# x is based on the sign of the decision function:\n",
    "\n",
    "# f(x)=w⋅x+b\n",
    "# f(x)>0, the data point is classified as one class (say +1).\n",
    "# f(x)<0, the data point is classified as the other class (say -1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af065fb4-ae82-4c6a-8a51-e31c149e2500",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfea84c-2d32-4f00-ad55-58f1b7913a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans.2 The objective function of a linear Support Vector Machine (SVM) is designed to maximize the margin between the two classes while minimizing classification errors. This can be divided into two main formulations depending on whether the data is linearly separable or not.\n",
    "\n",
    "# 1. Objective Function for Hard Margin SVM (Linearly Separable Data):\n",
    "# For linearly separable data, the SVM's objective is to maximize the margin between the two classes. The margin is defined as the distance between the decision boundary (hyperplane) and the closest data points (support vectors) of any class.\n",
    "\n",
    "# The objective function to maximize the margin can be written as:\n",
    " \n",
    "# subject to the constraints:\n",
    "#  +b)≥1,∀i\n",
    "# w is the weight vector.\n",
    "# b is the bias term.\n",
    "#  i is the label of the \n",
    "# ith data point (\n",
    " # ∈{−1,+1}).\n",
    " #  is the feature vector of the \n",
    "# ith data point.\n",
    "\n",
    "# 2. Objective Function for Soft Margin SVM (Non-Linearly Separable Data):\n",
    "# In cases where the data is not perfectly linearly separable, a soft margin SVM is used. The soft margin SVM introduces slack variables \n",
    "#  to allow some misclassification while still trying to maximize the margin.\n",
    "    \n",
    "# Explanation:\n",
    "# ∥w∥ 2 seeks to minimize the magnitude of the weight vector \n",
    "# w, which corresponds to maximizing the margin between the classes.\n",
    " #  penalizes the sum of slack variables, which represents the degree of misclassification or the distance of the misclassified points from the correct side of the margin.\n",
    "# Thus, the objective function of a linear SVM balances maximizing the margin and minimizing classification errors, with the regularization parameter \n",
    "# C controlling the trade-off between these two objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a72d11-f46c-4ea1-b63c-bf5884b9dad7",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM? write only theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ab333cd-cc6c-464d-92c2-f3095db957bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans.3 The kernel trick in Support Vector Machines (SVM) is a technique used to handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional space, where a linear separator (hyperplane) can be found.\n",
    "\n",
    "# Instead of explicitly transforming the data to this higher-dimensional space, the kernel trick allows SVM to operate in the original input space by using a kernel \n",
    "# function. The kernel function computes the dot product between the images of the data points in the higher-dimensional space, enabling the\n",
    "# SVM to find the optimal decision boundary without ever computing the coordinates in that space.\n",
    "\n",
    "# Common kernel functions include:\n",
    "\n",
    "# Linear Kernel:\n",
    " \n",
    "# Polynomial Kernel:\n",
    " \n",
    "# Radial Basis Function (RBF) Kernel/Gaussian Kernel: \n",
    "# The kernel trick is powerful because it allows SVMs to solve complex classification problems in higher-dimensional spaces without the computational cost of explicitly transforming the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772877e-74bf-47d0-aaad-2263bb7dd144",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b982de-6d8d-41a8-8c96-1d50314697da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans.4 Support vectors play a crucial role in Support Vector Machines (SVMs). They are the data points that are closest to the decision boundary (or hyperplane) and are the most difficult to classify. The position of these points determines the orientation and position of the decision boundary, and they are the key elements that define the margin between the two classes.\n",
    "\n",
    "# Role of Support Vectors:\n",
    "# Determining the Decision Boundary:\n",
    "\n",
    "# In SVM, the goal is to find a hyperplane that maximizes the margin between two classes. The margin is defined as the distance between the hyperplane and the closest data points from either class. These closest points are the support vectors.\n",
    "# The decision boundary is solely determined by these support vectors. If the other data points are removed or moved slightly, as long as they do not become support vectors, the decision boundary will not change.\n",
    "# Maximizing the Margin:\n",
    "\n",
    "# The optimal hyperplane is the one that maximizes the distance to the support vectors, hence maximizing the margin.\n",
    "# By maximizing this margin, SVM ensures that the model has the best possible generalization ability to classify new data points.\n",
    "# Influence on the SVM Model:\n",
    "\n",
    "# Only the support vectors are used in constructing the SVM model. Non-support vectors (data points that are not closest to the hyperplane) do not influence the final decision boundary.\n",
    "# This makes SVM computationally efficient, as it reduces the number of points that need to be considered in the optimization problem.\n",
    "# Example:\n",
    "# Imagine a simple 2D binary classification problem where we have two classes: Class A (represented by blue points) and Class B (represented by red points).\n",
    "\n",
    "# Case 1: Linearly Separable Data:\n",
    "\n",
    "# In this scenario, let's say we have several data points for each class. SVM will try to find a straight line (hyperplane) that separates the two classes with the maximum margin.\n",
    "# The support vectors will be the points from each class that are closest to this line. These points will lie on the margin boundaries, which are parallel lines on either side of the decision boundary.\n",
    "# Case 2: Non-Linearly Separable Data (Using Kernel Trick):\n",
    "\n",
    "# If the data is not linearly separable, SVM may use a kernel trick to transform the data into a higher-dimensional space where it becomes linearly separable.\n",
    "# Even in this transformed space, the support vectors are the critical points that determine the optimal hyperplane. These are the points that, after transformation, lie closest to the decision boundary.\n",
    "# Visualization:\n",
    "# In a simple 2D space, if you imagine a line separating two clusters of points, the points that are closest to this line are the support vectors.\n",
    "# Moving any of the non-support vectors will not change the position of this line, but moving a support vector will alter the line's orientation or position, hence changing the SVM model.\n",
    "# Conclusion:\n",
    "# Support vectors are the backbone of the SVM algorithm. They define the optimal decision boundary by determining the maximum margin between the classes. Their significance lies in the fact that the entire model relies on these critical data points, making SVM both effective and efficient in handling classification problems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a29b74-03d0-48e3-baa1-20533642508c",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a73c3-7ba0-4a9c-9a70-66e42d1f473c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
